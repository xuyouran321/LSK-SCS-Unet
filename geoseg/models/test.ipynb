{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "8700\n",
      "2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "#检测cuda能用吗\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda) #查看cuda\n",
    "print(torch.backends.cudnn.version()) #查看cudnn版本2.0.1\n",
    "#查看pytorch版本\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3090', major=8, minor=6, total_memory=24259MB, multi_processor_count=82)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_properties(1))  # 0 表示第一个GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.87375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 90.70 + 77.03 + 77.86 + 68.15 + 73.31 + 68.92 + 49.22 + 61.80\n",
    "a/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "except:\n",
    "    print(\"fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_left: torch.Size([4, 64, 64, 32])\n",
      "input_right: torch.Size([4, 64, 64, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[4, 64, 64, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 551\u001b[0m\n\u001b[1;32m    548\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# 前向传递输入张量通过ConvSSM模块\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# 打印输入和输出张量的尺寸\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 527\u001b[0m, in \u001b[0;36mConvSSM.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_left:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_left\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_right:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_right\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m--> 527\u001b[0m x \u001b[38;5;241m=\u001b[39m input_right \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_right\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    529\u001b[0m input_left \u001b[38;5;241m=\u001b[39m input_left\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[4, 64, 64, 32]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "except:\n",
    "    print(\"fail\")\n",
    "\n",
    "# an alternative for mamba_ssm (in which causal_conv1d is needed)\n",
    "try:\n",
    "    from selective_scan import selective_scan_fn as selective_scan_fn_v1\n",
    "    from selective_scan import selective_scan_ref as selective_scan_ref_v1\n",
    "except:\n",
    "    pass\n",
    "\n",
    "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
    "\n",
    "\"\"\"\n",
    "CNN在远程建模能力方面的局限性使它们无法有效地提取图像中的特征，而Transformers则受到其二次计算复杂性的阻碍。最近的研究表明，以Mamba为代表的状态空间模型（SSM）可以在保持线性计算复杂度的同时有效地模拟长程相互作用。\n",
    "我们介绍了一个新颖的Conv-SSM模块。Conv-SSM将卷积层的局部特征提取能力与SSM捕获长程依赖性的能力相结合。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):\n",
    "    \"\"\"\n",
    "    u: r(B D L)\n",
    "    delta: r(B D L)\n",
    "    A: r(D N)\n",
    "    B: r(B N L)\n",
    "    C: r(B N L)\n",
    "    D: r(D)\n",
    "    z: r(B D L)\n",
    "    delta_bias: r(D), fp32\n",
    "\n",
    "    ignores:\n",
    "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # fvcore.nn.jit_handles\n",
    "    def get_flops_einsum(input_shapes, equation):\n",
    "        np_arrs = [np.zeros(s) for s in input_shapes]\n",
    "        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n",
    "        for line in optim.split(\"\\n\"):\n",
    "            if \"optimized flop\" in line.lower():\n",
    "                # divided by 2 because we count MAC (multiply-add counted as one flop)\n",
    "                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n",
    "                return flop\n",
    "\n",
    "    assert not with_complex\n",
    "\n",
    "    flops = 0  # below code flops = 0\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        dtype_in = u.dtype\n",
    "        u = u.float()\n",
    "        delta = delta.float()\n",
    "        if delta_bias is not None:\n",
    "            delta = delta + delta_bias[..., None].float()\n",
    "        if delta_softplus:\n",
    "            delta = F.softplus(delta)\n",
    "        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
    "        is_variable_B = B.dim() >= 3\n",
    "        is_variable_C = C.dim() >= 3\n",
    "        if A.is_complex():\n",
    "            if is_variable_B:\n",
    "                B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
    "            if is_variable_C:\n",
    "                C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
    "        else:\n",
    "            B = B.float()\n",
    "            C = C.float()\n",
    "        x = A.new_zeros((batch, dim, dstate))\n",
    "        ys = []\n",
    "        \"\"\"\n",
    "\n",
    "    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n",
    "    if with_Group:\n",
    "        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\")\n",
    "    else:\n",
    "        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\")\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "        if not is_variable_B:\n",
    "            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
    "        else:\n",
    "            if B.dim() == 3:\n",
    "                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
    "            else:\n",
    "                B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
    "                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
    "        if is_variable_C and C.dim() == 4:\n",
    "            C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
    "        last_state = None\n",
    "        \"\"\"\n",
    "\n",
    "    in_for_flops = B * D * N\n",
    "    if with_Group:\n",
    "        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n",
    "    else:\n",
    "        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n",
    "    flops += L * in_for_flops\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        for i in range(u.shape[2]):\n",
    "            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
    "            if not is_variable_C:\n",
    "                y = torch.einsum('bdn,dn->bd', x, C)\n",
    "            else:\n",
    "                if C.dim() == 3:\n",
    "                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
    "                else:\n",
    "                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
    "            if i == u.shape[2] - 1:\n",
    "                last_state = x\n",
    "            if y.is_complex():\n",
    "                y = y.real * 2\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=2) # (batch dim L)\n",
    "        \"\"\"\n",
    "\n",
    "    if with_D:\n",
    "        flops += B * D * L\n",
    "    if with_Z:\n",
    "        flops += B * D * L\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
    "        if z is not None:\n",
    "            out = out * F.silu(z)\n",
    "        out = out.to(dtype=dtype_in)\n",
    "        \"\"\"\n",
    "\n",
    "    return flops\n",
    "\n",
    "\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging2D(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H // 2, W // 2, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchExpand2D(nn.Module):\n",
    "    def __init__(self, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim * 2\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(self.dim, dim_scale * self.dim, bias=False)\n",
    "        self.norm = norm_layer(self.dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale,\n",
    "                      c=C // self.dim_scale)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Final_PatchExpand2D(nn.Module):\n",
    "    def __init__(self, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(self.dim, dim_scale * self.dim, bias=False)\n",
    "        self.norm = norm_layer(self.dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale,\n",
    "                      c=C // self.dim_scale)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SS2D(\n",
    "    nn.Module):  # SS2D（State Space Model 2D）类，它是实现状态空间模型（SSM）在视觉任务中应用的核心部分。这个类将状态空间模型的长程依赖性建模能力与卷积操作结合起来，旨在捕获图像中的长程相互作用，同时保持对局部特征的敏感度。\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            d_state=16,\n",
    "            # d_state=\"auto\", # 20240109\n",
    "            d_conv=3,\n",
    "            expand=2,\n",
    "            dt_rank=\"auto\",\n",
    "            dt_min=0.001,\n",
    "            dt_max=0.1,\n",
    "            dt_init=\"random\",\n",
    "            dt_scale=1.0,\n",
    "            dt_init_floor=1e-4,\n",
    "            dropout=0.,\n",
    "            conv_bias=True,\n",
    "            bias=False,\n",
    "            device=None,\n",
    "            dtype=None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            groups=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            padding=(d_conv - 1) // 2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = (\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "        )\n",
    "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0))  # (K=4, N, inner)\n",
    "        del self.x_proj\n",
    "\n",
    "        self.dt_projs = (\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor,\n",
    "                         **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor,\n",
    "                         **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor,\n",
    "                         **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor,\n",
    "                         **factory_kwargs),\n",
    "        )\n",
    "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0))  # (K=4, inner, rank)\n",
    "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0))  # (K=4, inner)\n",
    "        del self.dt_projs\n",
    "\n",
    "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True)  # (K=4, D, N)\n",
    "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True)  # (K=4, D, N)\n",
    "\n",
    "        # self.selective_scan = selective_scan_fn\n",
    "        self.forward_core = self.forward_corev0\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
    "\n",
    "    @staticmethod\n",
    "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4,\n",
    "                **factory_kwargs):\n",
    "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = dt_rank ** -0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        dt_proj.bias._no_reinit = True\n",
    "\n",
    "        return dt_proj\n",
    "\n",
    "    @staticmethod\n",
    "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        if copies > 1:\n",
    "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
    "            if merge:\n",
    "                A_log = A_log.flatten(0, 1)\n",
    "        A_log = nn.Parameter(A_log)\n",
    "        A_log._no_weight_decay = True\n",
    "        return A_log\n",
    "\n",
    "    @staticmethod\n",
    "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
    "        # D \"skip\" parameter\n",
    "        D = torch.ones(d_inner, device=device)\n",
    "        if copies > 1:\n",
    "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
    "            if merge:\n",
    "                D = D.flatten(0, 1)\n",
    "        D = nn.Parameter(D)  # Keep in fp32\n",
    "        D._no_weight_decay = True\n",
    "        return D\n",
    "\n",
    "    def forward_corev0(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)],\n",
    "                             dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1)  # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L)  # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L)  # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1)  # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1)  # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts,\n",
    "            As, Bs, Cs, Ds, z=None,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=False,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    # an alternative to forward_corev1\n",
    "    def forward_corev1(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn_v1\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)],\n",
    "                             dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1)  # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L)  # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L)  # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1)  # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1)  # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts,\n",
    "            As, Bs, Cs, Ds,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x, z = xz.chunk(2, dim=-1)  # (b, h, w, d)\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.act(self.conv2d(x))  # (b, d, h, w)\n",
    "        y1, y2, y3, y4 = self.forward_core(x)\n",
    "        assert y1.dtype == torch.float32\n",
    "        y = y1 + y2 + y3 + y4\n",
    "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
    "        y = self.out_norm(y)\n",
    "        y = y * F.silu(z)\n",
    "        out = self.out_proj(y)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    这个类组合了自注意力机制和卷积操作，旨在融合自注意力的全局感知能力和卷积的局部特征提取能力。\n",
    "    输入特征被分成两部分，一部分通过SS2D自注意力模块处理，另一部分通过一系列卷积层处理。处理后的两部分再次合并，并通过最终的卷积层生成输出特征。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dim: int = 0,\n",
    "            drop_path: float = 0,\n",
    "            norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "            attn_drop_rate: float = 0,\n",
    "            d_state: int = 16,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(hidden_dim // 2)\n",
    "        self.self_attention = SS2D(d_model=hidden_dim // 2, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "        self.conv33conv33conv11 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_dim // 2, out_channels=hidden_dim // 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim // 2, out_channels=hidden_dim // 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim // 2, out_channels=hidden_dim // 2, kernel_size=1, stride=1)\n",
    "        )\n",
    "        self.finalconv11 = nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        input_left, input_right = input.chunk(2, dim=-1)\n",
    "        print(\"input_left:\", input_left.size())\n",
    "        print(\"input_right:\", input_right.size())\n",
    "        x = input_right + self.drop_path(self.self_attention(self.ln_1(input_right)))\n",
    "        print(\"x:\", x.size())\n",
    "        input_left = input_left.permute(0, 3, 1, 2).contiguous()\n",
    "        print(\"input_left:\", input_left.size())\n",
    "        input_left = self.conv33conv33conv11(input_left)\n",
    "        print(\"input_left:\", input_left.size())\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        output = torch.cat((input_left, x), dim=1)\n",
    "        output = self.finalconv11(output).permute(0, 2, 3, 1).contiguous()\n",
    "        print(\"output:\", output.size())\n",
    "        print(\"input:\", input.size())\n",
    "        return output + input\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 初始化ConvSSM模块，hidden_dim为128\n",
    "    block = ConvSSM(hidden_dim=128).to(device)\n",
    "\n",
    "    # 生成随机输入张量，尺寸为[批次大小, 高度, 宽度, 通道数]\n",
    "    # 这里批次大小为4，高度和宽度为32，通道数为128（符合hidden_dim的大小）\n",
    "    input_tensor = torch.rand(4, 32, 32, 128).to(device)\n",
    "\n",
    "    # 前向传递输入张量通过ConvSSM模块\n",
    "    output = block(input_tensor)\n",
    "\n",
    "    # 打印输入和输出张量的尺寸\n",
    "    print(\"Input tensor size:\", input_tensor.size())\n",
    "    print(\"Output tensor size:\", output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data/uavid/test_val/images/seq41_000000_0_4.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq41_000400_0_4.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq41_000500_0_4.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000600_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq41_000700_0_4.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000300_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000900_0_4.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000700_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq41_000800_0_4.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000500_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000400_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq41_000100_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000100_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000900_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000200_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq41_000000_0_0.png size: 1024 x 1024\n",
      "Image data/uavid/test_val/images/seq23_000000_0_0.png size: 1024 x 1024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# 指定图像文件夹路径\n",
    "folder_path = 'data/uavid/test_val/images'\n",
    "\n",
    "# 获取文件夹中所有图像文件的路径\n",
    "image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "# 遍历每张图像文件并输出尺寸\n",
    "for image_path in image_paths:\n",
    "    # 读取图像\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is not None:\n",
    "        # 获取图像尺寸\n",
    "        height, width, channels = image.shape\n",
    "        \n",
    "        # 输出图像尺寸\n",
    "        print(f\"Image {image_path} size: {width} x {height}\")\n",
    "    else:\n",
    "        print(f\"Error reading image {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seq22_000900_0_0.png', 'seq22_000900_0_1.png', 'seq22_000900_0_2.png', 'seq22_000900_0_3.png', 'seq22_000900_0_4.png', 'seq22_000900_0_5.png', 'seq22_000900_0_6.png', 'seq22_000900_0_7.png', 'seq30_000900_0_0.png', 'seq30_000900_0_1.png', 'seq30_000900_0_2.png', 'seq30_000900_0_3.png', 'seq30_000900_0_4.png', 'seq30_000900_0_5.png', 'seq30_000900_0_6.png', 'seq30_000900_0_7.png']\n",
      "seq22000900 ['output/all/patch/seq22_000900_0_0.png', 'output/all/patch/seq22_000900_0_1.png', 'output/all/patch/seq22_000900_0_2.png', 'output/all/patch/seq22_000900_0_3.png', 'output/all/patch/seq22_000900_0_4.png', 'output/all/patch/seq22_000900_0_5.png', 'output/all/patch/seq22_000900_0_6.png', 'output/all/patch/seq22_000900_0_7.png']\n",
      "seq30000900 ['output/all/patch/seq30_000900_0_0.png', 'output/all/patch/seq30_000900_0_1.png', 'output/all/patch/seq30_000900_0_2.png', 'output/all/patch/seq30_000900_0_3.png', 'output/all/patch/seq30_000900_0_4.png', 'output/all/patch/seq30_000900_0_5.png', 'output/all/patch/seq30_000900_0_6.png', 'output/all/patch/seq30_000900_0_7.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# 定义每组图像的大小和行列数\n",
    "image_size = (1024, 1024)\n",
    "rows = 2\n",
    "columns = 4\n",
    "\n",
    "# 定义文件夹路径\n",
    "input_folder = \"output/all/patch\"\n",
    "output_folder = \"output/all/concat\"\n",
    "\n",
    "# 创建保存拼接图像的文件夹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "def concat_images():\n",
    "    # 初始化字典来存储每组图像的名称和路径\n",
    "    image_groups = {}\n",
    "    # 获取输入文件夹中的所有图像文件并进行排序\n",
    "    image_files = sorted(os.listdir(input_folder))\n",
    "    print(image_files)\n",
    "    # 遍历图像文件列表并将图像按组存储到字典中\n",
    "    for image_file in image_files:\n",
    "        image_name = image_file.split(\"_\")[0]+image_file.split(\"_\")[1]\n",
    "        if image_name not in image_groups:\n",
    "            image_groups[image_name] = []\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        image_groups[image_name].append(image_path)\n",
    "    return image_groups\n",
    "\n",
    "\n",
    "# 定义函数来拼接图像\n",
    "def concatenate_images(image_paths):\n",
    "    # 创建一个空白画布来存放拼接后的图像\n",
    "    concat_image = Image.new(\"L\", (columns * image_size[0], rows * image_size[1]))\n",
    "    \n",
    "    # 遍历每张图像的路径和对应的行列位置并进行拼接\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        row = i // columns\n",
    "        column = i % columns\n",
    "        image = Image.open(image_path)\n",
    "        concat_image.paste(image, (column * image_size[0], row * image_size[1]))\n",
    "    \n",
    "    return concat_image\n",
    "\n",
    "\n",
    "image_groups = concat_images()\n",
    "for image_name, image_paths in image_groups.items():\n",
    "    print(image_name,image_paths)\n",
    "#     concat_image = concatenate_images(image_paths)\n",
    "#     output_name = image_name + \".png\"\n",
    "#     output_path = os.path.join(output_folder, output_name)\n",
    "#     concat_image.save(output_path)\n",
    "# print(\"图像拼接完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [  0 128   0]\n",
      "  [  0 128   0]\n",
      "  [  0 128   0]]\n",
      "\n",
      " [[128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [  0 128   0]\n",
      "  [  0 128   0]\n",
      "  [  0 128   0]]\n",
      "\n",
      " [[128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [  0 128   0]\n",
      "  [  0 128   0]\n",
      "  [  0 128   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[128  64 128]\n",
      "  [128  64 128]\n",
      "  [128  64 128]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " [[128  64 128]\n",
      "  [128  64 128]\n",
      "  [128  64 128]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " [[128  64 128]\n",
      "  [128  64 128]\n",
      "  [128  64 128]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 打开图像文件\n",
    "img = Image.open('output/all/patch/seq30_000900_0_3.png')\n",
    "\n",
    "# 将图像转换为NumPy数组\n",
    "img_array = np.array(img)\n",
    "\n",
    "# 打印数组\n",
    "print(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# 输入 N C H W,  输出 N C H W\n",
    "if __name__ == '__main__':\n",
    "    input = torch.randn(50, 512, 7, 7)\n",
    "    se = ChannelAttentionModule(in_channels=512)\n",
    "    output = se(input)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(50, 512, 1, 1)\n",
    "se = SpatialAttentionModule()\n",
    "output = se(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (179200x7 and 512x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     78\u001b[0m se \u001b[38;5;241m=\u001b[39m ShuffleAttention(channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, G\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 69\u001b[0m, in \u001b[0;36mShuffleAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_shuffle(out, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# 应用全连接层将通道数从512减少到1\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (179200x7 and 512x1)"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ShuffleAttention(nn.Module):\n",
    "    # 初始化Shuffle Attention模块\n",
    "    def __init__(self, channel=512, reduction=16, G=8):\n",
    "        super().__init__()\n",
    "        self.G = G  # 分组数量\n",
    "        self.channel = channel  # 通道数\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # 全局平均池化，用于生成通道注意力\n",
    "        self.gn = nn.GroupNorm(channel // (2 * G), channel // (2 * G))  # 分组归一化，用于空间注意力\n",
    "        # 以下为通道注意力和空间注意力的权重和偏置参数\n",
    "        self.cweight = Parameter(torch.zeros(1, channel // (2 * G), 1, 1))\n",
    "        self.cbias = Parameter(torch.ones(1, channel // (2 * G), 1, 1))\n",
    "        self.sweight = Parameter(torch.zeros(1, channel // (2 * G), 1, 1))\n",
    "        self.sbias = Parameter(torch.ones(1, channel // (2 * G), 1, 1))\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid函数，用于生成注意力图\n",
    "        self.fc = nn.Linear(channel, 1)  # 添加全连接层将通道数从512减少到1\n",
    "\n",
    "\n",
    "    # 权重初始化方法\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    # 通道混洗方法，用于在分组处理后重组特征\n",
    "    @staticmethod\n",
    "    def channel_shuffle(x, groups):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.reshape(b, groups, -1, h, w)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(b, -1, h, w)\n",
    "        return x\n",
    "\n",
    "    # 前向传播方法\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b * self.G, -1, h, w)  # 将输入特征图按照分组维度进行重排\n",
    "\n",
    "        x_0, x_1 = x.chunk(2, dim=1)  # 将特征图分为两部分，分别用于通道注意力和空间注意力\n",
    "\n",
    "        # 通道注意力分支\n",
    "        x_channel = self.avg_pool(x_0)  # 对第一部分应用全局平均池化\n",
    "        x_channel = self.cweight * x_channel + self.cbias  # 应用学习到的权重和偏置\n",
    "        x_channel = x_0 * self.sigmoid(x_channel)  # 通过sigmoid激活函数和原始特征图相乘，得到加权的特征图\n",
    "\n",
    "        # 空间注意力分支\n",
    "        x_spatial = self.gn(x_1)  # 对第二部分应用分组归一化\n",
    "        x_spatial = self.sweight * x_spatial + self.sbias  # 应用学习到的权重和偏置\n",
    "        x_spatial = x_1 * self.sigmoid(x_spatial)  # 通过sigmoid激活函数和原始特征图相乘，得到加权的特征图\n",
    "\n",
    "        # 将通道注意力和空间注意力的结果沿通道维度拼接\n",
    "        out = torch.cat([x_channel, x_spatial], dim=1)\n",
    "        out = out.contiguous().view(b, -1, h, w)  # 重新调整形状以匹配原始输入的维度\n",
    "\n",
    "        # 应用通道混洗，以便不同分组间的特征可以交换信息\n",
    "        out = self.channel_shuffle(out, 2)\n",
    "\n",
    "        # 应用全连接层将通道数从512减少到1\n",
    "        out = self.fc(out)\n",
    "        out = out.view(b, 1, 1, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# 输入 N C H W,  输出 N C H W\n",
    "if __name__ == '__main__':\n",
    "    input = torch.randn(50, 512, 7, 7)\n",
    "    se = ShuffleAttention(channel=512, G=4)\n",
    "    output = se(input)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [8, 7, 1, 1], expected input[50, 512, 14, 1] to have 7 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m)  \u001b[38;5;66;03m# 创建一个随机输入\u001b[39;00m\n\u001b[1;32m     67\u001b[0m block \u001b[38;5;241m=\u001b[39m CoordAtt(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 实例化Coordinate Attention模块\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 通过模块处理输入\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 打印输入和输出的尺寸\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 50\u001b[0m, in \u001b[0;36mCoordAtt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w(x)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 垂直方向池化并交换维度以适应拼接\u001b[39;00m\n\u001b[1;32m     49\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_h, x_w], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 拼接水平和垂直方向的特征\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 通过1x1卷积降维\u001b[39;00m\n\u001b[1;32m     51\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(y)  \u001b[38;5;66;03m# 批归一化\u001b[39;00m\n\u001b[1;32m     52\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(y)  \u001b[38;5;66;03m# 激活函数\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/airs/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 7, 1, 1], expected input[50, 512, 14, 1] to have 7 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义h_sigmoid激活函数，这是一种硬Sigmoid函数\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)  # 使用ReLU6实现\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6  # 公式为ReLU6(x+3)/6，模拟Sigmoid激活函数\n",
    "\n",
    "# 定义h_swish激活函数，这是基于h_sigmoid的Swish函数变体\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)  # 使用上面定义的h_sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)  # 公式为x * h_sigmoid(x)\n",
    "\n",
    "# 定义Coordinate Attention模块\n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        # 定义水平和垂直方向的自适应平均池化\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))  # 水平方向\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))  # 垂直方向\n",
    "\n",
    "        mip = max(8, inp // reduction)  # 计算中间层的通道数\n",
    "\n",
    "        # 1x1卷积用于降维\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)  # 批归一化\n",
    "        self.act = h_swish()  # 激活函数\n",
    "\n",
    "        # 两个1x1卷积，分别对应水平和垂直方向\n",
    "        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # 保存输入作为残差连接\n",
    "\n",
    "        n, c, h, w = x.size()  # 获取输入的尺寸\n",
    "        x_h = self.pool_h(x)  # 水平方向池化\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)  # 垂直方向池化并交换维度以适应拼接\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)  # 拼接水平和垂直方向的特征\n",
    "        y = self.conv1(y)  # 通过1x1卷积降维\n",
    "        y = self.bn1(y)  # 批归一化\n",
    "        y = self.act(y)  # 激活函数\n",
    "\n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)  # 将特征拆分回水平和垂直方向\n",
    "        x_w = x_w.permute(0, 1, 3, 2)  # 恢复x_w的原始维度\n",
    "\n",
    "        a_h = self.conv_h(x_h).sigmoid()  # 通过1x1卷积并应用Sigmoid获取水平方向的注意力权重\n",
    "        a_w = self.conv_w(x_w).sigmoid()  # 通过1x1卷积并应用Sigmoid获取垂直方向的注意力权重\n",
    "\n",
    "        out = identity * a_w * a_h  # 应用注意力权重到输入特征，并与残差连接相乘\n",
    "\n",
    "        return out  # 返回输出\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    input = torch.randn(50, 512, 7, 7)  # 创建一个随机输入\n",
    "    block = CoordAtt(input.shape[2], input.shape[3])  # 实例化Coordinate Attention模块\n",
    "    output = block(input)  # 通过模块处理输入\n",
    "    print(output.shape)  # 打印输入和输出的尺寸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(50, 512, 7, 7)  # 创建一个随机输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
